{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from tool import *\n",
    "\n",
    "data_root = '../../MyData/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data (With GICS_Sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of item1_embedding: 1197\n",
      "Length of other_embedding: 1197\n",
      "1197\n",
      "The nan value proportion in each column:\n",
      "cik                           0.000000\n",
      "tic                           0.000000\n",
      "Year                          0.000000\n",
      "item1_embeddings              0.000000\n",
      "GICS_Sector                   0.006683\n",
      "SP_SHORT_DESC_embeddings      0.041771\n",
      "SP_LONG_DESC_embeddings       0.095238\n",
      "ORBIS_PROD_SERV_embeddings    0.208020\n",
      "ORBIS_OVERVIEW_embeddings     0.168755\n",
      "dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_embedding = obtain_total_embedding(data_root)\n",
    "print(len(total_embedding))\n",
    "\n",
    "# Check dataset\n",
    "nan_proportion = total_embedding.isna().mean()\n",
    "print(f\"The nan value proportion in each column:\\n{nan_proportion}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.obtain_model import load_ae\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "trained_ae = load_ae(\"../model/saved_models/basic_ae.pth\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming x is a batch of input data (1D, not images)\n",
    "# x = next(iter(test_loader))  # Get a batch of data\n",
    "# x = x[0][8:16]\n",
    "\n",
    "# # Encode the input\n",
    "# trained_ae.eval()\n",
    "# with torch.no_grad():\n",
    "#     x = x.unsqueeze(1)\n",
    "#     generated_samples = trained_ae(x.to(device))\n",
    "    \n",
    "#     # Move to CPU and convert to numpy if necessary\n",
    "#     x = x.cpu()\n",
    "#     x = x.squeeze(1)\n",
    "#     print(x.shape)\n",
    "#     generated_samples = generated_samples.cpu()\n",
    "#     generated_samples = generated_samples.squeeze(1)\n",
    "#     print(generated_samples.shape)\n",
    "    \n",
    "#     # Plot the original and generated samples in separate subplots\n",
    "#     fig, axes = plt.subplots(8, 1, figsize=(12, 24))  # 8 rows, 1 column\n",
    "    \n",
    "#     for i in range(8):\n",
    "#         axes[i].plot(x[i][900:1100], label=\"Original\", linestyle='-', color='b', alpha=0.7)\n",
    "#         axes[i].plot(generated_samples[i][900:1100], label=\"Generated\", linestyle='--', color='r', alpha=0.7)\n",
    "#         axes[i].set_title(f\"Sample {i+1}\")\n",
    "#         axes[i].set_xlabel(\"Data Dimension\")\n",
    "#         axes[i].set_ylabel(\"Value\")\n",
    "#         axes[i].legend()\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(1197, 9)\n",
      "total number of embedding: 5985\n"
     ]
    }
   ],
   "source": [
    "target_list = ['item1_embeddings', 'SP_LONG_DESC_embeddings', 'SP_SHORT_DESC_embeddings', 'ORBIS_PROD_SERV_embeddings', 'ORBIS_OVERVIEW_embeddings']\n",
    "info_list = ['cik', 'tic', 'Year', 'GICS_Sector']\n",
    "\n",
    "exp_df = convert_to_array(total_embedding, info_list, target_list, 1536, False)\n",
    "print(type(exp_df))\n",
    "print(exp_df.shape)\n",
    "\n",
    "\n",
    "# Dictionary to store the stacked embeddings as PyTorch tensors\n",
    "embedding_tensors = {}\n",
    "\n",
    "# Loop through the columns and convert to PyTorch tensor\n",
    "for col in target_list:\n",
    "    numpy_array = np.vstack(exp_df[col].values)  # Stack the column values\n",
    "    embedding_tensors[col] = torch.tensor(numpy_array, dtype=torch.float32)  # Convert to tensor\n",
    "\n",
    "# embedding_tensors is a dictionary containing all the openai embeddings I have.\n",
    "emb_num = 0\n",
    "for col in target_list:\n",
    "    emb_num += len(embedding_tensors[col])\n",
    "print(f\"total number of embedding: {emb_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'item1_embeddings': tensor([[-0.5898,  0.9802, -0.9664,  ...,  0.9971,  0.9972, -0.3560],\n",
      "        [ 0.9821,  0.4754,  0.8698,  ...,  0.9983,  0.9933,  0.3218],\n",
      "        [ 0.9946, -0.4087,  0.6804,  ...,  0.9944,  0.9053,  0.3607],\n",
      "        ...,\n",
      "        [-0.3125,  0.9592,  0.8852,  ...,  0.9983,  0.9903,  0.9140],\n",
      "        [-0.6024,  0.8309,  0.6144,  ...,  0.9998,  0.8273, -0.8432],\n",
      "        [ 0.5317,  0.9640,  0.9502,  ..., -0.9994, -0.1773,  0.8744]],\n",
      "       device='cuda:0'), 'SP_LONG_DESC_embeddings': tensor([[-0.8353,  0.9734, -0.9823,  ...,  0.9971,  0.9976, -0.7578],\n",
      "        [ 0.9717,  0.7132,  0.8791,  ...,  0.9911,  0.8676, -0.7440],\n",
      "        [ 0.9719, -0.3351,  0.9141,  ...,  0.9956,  0.9862,  0.3395],\n",
      "        ...,\n",
      "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [-0.1758,  0.9904,  0.8948,  ..., -0.9995, -0.1103,  0.8007]],\n",
      "       device='cuda:0'), 'SP_SHORT_DESC_embeddings': tensor([[-0.6867,  0.9938, -0.9920,  ...,  0.9990,  0.9984, -0.8257],\n",
      "        [ 0.9754,  0.2401,  0.8951,  ...,  0.9970,  0.9942, -0.1940],\n",
      "        [ 0.9910, -0.3897,  0.1733,  ...,  0.9938,  0.9887,  0.4006],\n",
      "        ...,\n",
      "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [-0.3860,  0.9968,  0.8220,  ..., -0.9989, -0.7166,  0.4519]],\n",
      "       device='cuda:0'), 'ORBIS_PROD_SERV_embeddings': tensor([[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [ 0.9728,  0.9936,  0.9686,  ...,  0.9983,  0.9880, -0.1903],\n",
      "        [ 0.9950, -0.8198,  0.8770,  ...,  0.9687, -0.7578, -0.7438],\n",
      "        ...,\n",
      "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [-0.7989,  0.9828,  0.1794,  ...,  0.3016, -0.9985,  0.8141]],\n",
      "       device='cuda:0'), 'ORBIS_OVERVIEW_embeddings': tensor([[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [ 0.8935,  0.9151,  0.7551,  ...,  0.9825, -0.0444, -0.9134],\n",
      "        [ 0.9968, -0.3648,  0.7675,  ...,  0.9925,  0.9506, -0.0904],\n",
      "        ...,\n",
      "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [-0.1153,  0.9665, -0.0244,  ..., -0.9805, -0.9381,  0.8755]],\n",
      "       device='cuda:0')}\n",
      "total number of embedding: 5985\n"
     ]
    }
   ],
   "source": [
    "latent_tensors = {}\n",
    "for col in target_list:\n",
    "    latent_tensors[col] = safe_inference(trained_ae.encoder_net, embedding_tensors[col].to(device))\n",
    "\n",
    "print(latent_tensors)\n",
    "\n",
    "emb_num = 0\n",
    "for col in target_list:\n",
    "    emb_num += len(latent_tensors[col])\n",
    "print(f\"total number of embedding: {emb_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in target_list:\n",
    "    exp_df[col] = latent_tensors[col].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_df = pd.read_csv(\"./data/embedding_256.csv\")\n",
    "# exp_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
