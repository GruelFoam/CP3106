{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from tool import *\n",
    "\n",
    "data_root = '../../MyData/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data (With GICS_Sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1197\n",
      "       cik   tic  Year                                   item1_embeddings  \\\n",
      "0   850460  WIRE  2021  [0.03955410048365593, -0.04159577935934067, -0...   \n",
      "1   352541   LNT  2021  [-0.0231856107711792, 0.001279839314520359, 0....   \n",
      "2  1704715   AMR  2021  [0.014074714854359627, 0.006938479840755463, 0...   \n",
      "3  1575515   SFM  2021  [-0.023708730936050415, 0.01762891560792923, 0...   \n",
      "4  1125376  ENSG  2021  [0.04463111609220505, 0.0018805989529937506, 0...   \n",
      "\n",
      "   GICS_Sector  \n",
      "0         20.0  \n",
      "1         55.0  \n",
      "2         15.0  \n",
      "3         30.0  \n",
      "4         35.0  \n",
      "1197\n",
      "       cik                           SP_SHORT_DESC_embeddings  \\\n",
      "0   850460  [0.01568225771188736, -0.07636360824108124, -0...   \n",
      "1   352541  [-0.013836896046996117, -0.028995024040341377,...   \n",
      "2  1704715  [0.0030535957776010036, 0.0008783274097368121,...   \n",
      "3  1575515  [-0.025762900710105896, 0.0034106436651200056,...   \n",
      "4  1125376  [0.04616139456629753, -0.0021259395871311426, ...   \n",
      "\n",
      "                             SP_LONG_DESC_embeddings  \\\n",
      "0  [0.06089901179075241, -0.07059630751609802, -0...   \n",
      "1  [-0.021961161866784096, 0.020302705466747284, ...   \n",
      "2  [-0.0017570963827893138, 0.006557094398885965,...   \n",
      "3  [-0.015190708450973034, 0.011541897431015968, ...   \n",
      "4  [0.05935207009315491, 0.0013673527864739299, 0...   \n",
      "\n",
      "                          ORBIS_PROD_SERV_embeddings  \\\n",
      "0                                                NaN   \n",
      "1  [-0.015400929376482964, 0.03232719004154205, -...   \n",
      "2  [-0.02030838653445244, -0.009386725723743439, ...   \n",
      "3  [-0.013177500106394291, 0.036401789635419846, ...   \n",
      "4  [0.029405493289232254, 0.014010551385581493, 0...   \n",
      "\n",
      "                           ORBIS_OVERVIEW_embeddings  \n",
      "0                                                NaN  \n",
      "1  [-0.041709959506988525, -0.011898815631866455,...  \n",
      "2  [-0.012340557761490345, -0.01428779773414135, ...  \n",
      "3  [-0.01883152313530445, 0.021741388365626335, 0...  \n",
      "4  [-0.006068837363272905, 0.010537531226873398, ...  \n",
      "1197\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>tic</th>\n",
       "      <th>Year</th>\n",
       "      <th>item1_embeddings</th>\n",
       "      <th>GICS_Sector</th>\n",
       "      <th>SP_SHORT_DESC_embeddings</th>\n",
       "      <th>SP_LONG_DESC_embeddings</th>\n",
       "      <th>ORBIS_PROD_SERV_embeddings</th>\n",
       "      <th>ORBIS_OVERVIEW_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>850460</td>\n",
       "      <td>WIRE</td>\n",
       "      <td>2021</td>\n",
       "      <td>[0.03955410048365593, -0.04159577935934067, -0...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>[0.01568225771188736, -0.07636360824108124, -0...</td>\n",
       "      <td>[0.06089901179075241, -0.07059630751609802, -0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>352541</td>\n",
       "      <td>LNT</td>\n",
       "      <td>2021</td>\n",
       "      <td>[-0.0231856107711792, 0.001279839314520359, 0....</td>\n",
       "      <td>55.0</td>\n",
       "      <td>[-0.013836896046996117, -0.028995024040341377,...</td>\n",
       "      <td>[-0.021961161866784096, 0.020302705466747284, ...</td>\n",
       "      <td>[-0.015400929376482964, 0.03232719004154205, -...</td>\n",
       "      <td>[-0.041709959506988525, -0.011898815631866455,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1704715</td>\n",
       "      <td>AMR</td>\n",
       "      <td>2021</td>\n",
       "      <td>[0.014074714854359627, 0.006938479840755463, 0...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>[0.0030535957776010036, 0.0008783274097368121,...</td>\n",
       "      <td>[-0.0017570963827893138, 0.006557094398885965,...</td>\n",
       "      <td>[-0.02030838653445244, -0.009386725723743439, ...</td>\n",
       "      <td>[-0.012340557761490345, -0.01428779773414135, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1575515</td>\n",
       "      <td>SFM</td>\n",
       "      <td>2021</td>\n",
       "      <td>[-0.023708730936050415, 0.01762891560792923, 0...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>[-0.025762900710105896, 0.0034106436651200056,...</td>\n",
       "      <td>[-0.015190708450973034, 0.011541897431015968, ...</td>\n",
       "      <td>[-0.013177500106394291, 0.036401789635419846, ...</td>\n",
       "      <td>[-0.01883152313530445, 0.021741388365626335, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1125376</td>\n",
       "      <td>ENSG</td>\n",
       "      <td>2021</td>\n",
       "      <td>[0.04463111609220505, 0.0018805989529937506, 0...</td>\n",
       "      <td>35.0</td>\n",
       "      <td>[0.04616139456629753, -0.0021259395871311426, ...</td>\n",
       "      <td>[0.05935207009315491, 0.0013673527864739299, 0...</td>\n",
       "      <td>[0.029405493289232254, 0.014010551385581493, 0...</td>\n",
       "      <td>[-0.006068837363272905, 0.010537531226873398, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cik   tic  Year                                   item1_embeddings  \\\n",
       "0   850460  WIRE  2021  [0.03955410048365593, -0.04159577935934067, -0...   \n",
       "1   352541   LNT  2021  [-0.0231856107711792, 0.001279839314520359, 0....   \n",
       "2  1704715   AMR  2021  [0.014074714854359627, 0.006938479840755463, 0...   \n",
       "3  1575515   SFM  2021  [-0.023708730936050415, 0.01762891560792923, 0...   \n",
       "4  1125376  ENSG  2021  [0.04463111609220505, 0.0018805989529937506, 0...   \n",
       "\n",
       "   GICS_Sector                           SP_SHORT_DESC_embeddings  \\\n",
       "0         20.0  [0.01568225771188736, -0.07636360824108124, -0...   \n",
       "1         55.0  [-0.013836896046996117, -0.028995024040341377,...   \n",
       "2         15.0  [0.0030535957776010036, 0.0008783274097368121,...   \n",
       "3         30.0  [-0.025762900710105896, 0.0034106436651200056,...   \n",
       "4         35.0  [0.04616139456629753, -0.0021259395871311426, ...   \n",
       "\n",
       "                             SP_LONG_DESC_embeddings  \\\n",
       "0  [0.06089901179075241, -0.07059630751609802, -0...   \n",
       "1  [-0.021961161866784096, 0.020302705466747284, ...   \n",
       "2  [-0.0017570963827893138, 0.006557094398885965,...   \n",
       "3  [-0.015190708450973034, 0.011541897431015968, ...   \n",
       "4  [0.05935207009315491, 0.0013673527864739299, 0...   \n",
       "\n",
       "                          ORBIS_PROD_SERV_embeddings  \\\n",
       "0                                                NaN   \n",
       "1  [-0.015400929376482964, 0.03232719004154205, -...   \n",
       "2  [-0.02030838653445244, -0.009386725723743439, ...   \n",
       "3  [-0.013177500106394291, 0.036401789635419846, ...   \n",
       "4  [0.029405493289232254, 0.014010551385581493, 0...   \n",
       "\n",
       "                           ORBIS_OVERVIEW_embeddings  \n",
       "0                                                NaN  \n",
       "1  [-0.041709959506988525, -0.011898815631866455,...  \n",
       "2  [-0.012340557761490345, -0.01428779773414135, ...  \n",
       "3  [-0.01883152313530445, 0.021741388365626335, 0...  \n",
       "4  [-0.006068837363272905, 0.010537531226873398, ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item1_embedding = pd.read_csv(data_root+'merged_1197.csv')\n",
    "item1_embedding = item1_embedding[['cik', 'tic', 'Year', 'item1_embeddings', 'GICS_Sector']]\n",
    "print(len(item1_embedding))\n",
    "print(item1_embedding.head())\n",
    "\n",
    "other_embedding = pd.read_csv(data_root+'output_embeddings_2.csv')\n",
    "other_embedding = other_embedding[['cik', 'SP_SHORT_DESC_embeddings', 'SP_LONG_DESC_embeddings', 'ORBIS_PROD_SERV_embeddings', 'ORBIS_OVERVIEW_embeddings']]\n",
    "print(len(other_embedding))\n",
    "print(other_embedding.head())\n",
    "\n",
    "total_embedding = pd.merge(item1_embedding, other_embedding, on=['cik'])\n",
    "\n",
    "# To reduce mem consumption\n",
    "item1_embedding = ''\n",
    "other_embedding = ''\n",
    "\n",
    "print(len(total_embedding))\n",
    "total_embedding.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nan value proportion in each column:\n",
      "cik                           0.000000\n",
      "tic                           0.000000\n",
      "Year                          0.000000\n",
      "item1_embeddings              0.000000\n",
      "GICS_Sector                   0.006683\n",
      "SP_SHORT_DESC_embeddings      0.041771\n",
      "SP_LONG_DESC_embeddings       0.095238\n",
      "ORBIS_PROD_SERV_embeddings    0.208020\n",
      "ORBIS_OVERVIEW_embeddings     0.168755\n",
      "dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nan_proportion = total_embedding.isna().mean()\n",
    "print(f\"The nan value proportion in each column:\\n{nan_proportion}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.obtain_model import load_ae\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "trained_ae = load_ae(\"../model/saved_models/basic_ae.pth\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "(1197, 9)\n",
      "total number of embedding: 5985\n"
     ]
    }
   ],
   "source": [
    "target_list = ['item1_embeddings', 'SP_LONG_DESC_embeddings', 'SP_SHORT_DESC_embeddings', 'ORBIS_PROD_SERV_embeddings', 'ORBIS_OVERVIEW_embeddings']\n",
    "info_list = ['cik', 'tic', 'Year', 'GICS_Sector']\n",
    "\n",
    "exp_df = convert_to_array(total_embedding, info_list, target_list, 1536, False)\n",
    "print(type(exp_df))\n",
    "print(exp_df.shape)\n",
    "\n",
    "\n",
    "# Dictionary to store the stacked embeddings as PyTorch tensors\n",
    "embedding_tensors = {}\n",
    "\n",
    "# Loop through the columns and convert to PyTorch tensor\n",
    "for col in target_list:\n",
    "    numpy_array = np.vstack(exp_df[col].values)  # Stack the column values\n",
    "    embedding_tensors[col] = torch.tensor(numpy_array, dtype=torch.float32)  # Convert to tensor\n",
    "\n",
    "# embedding_tensors is a dictionary containing all the openai embeddings I have.\n",
    "emb_num = 0\n",
    "for col in target_list:\n",
    "    emb_num += len(embedding_tensors[col])\n",
    "print(f\"total number of embedding: {emb_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'item1_embeddings': tensor([[-0.5898,  0.9802, -0.9664,  ...,  0.9971,  0.9972, -0.3560],\n",
      "        [ 0.9821,  0.4754,  0.8698,  ...,  0.9983,  0.9933,  0.3218],\n",
      "        [ 0.9946, -0.4087,  0.6804,  ...,  0.9944,  0.9053,  0.3607],\n",
      "        ...,\n",
      "        [-0.3125,  0.9592,  0.8852,  ...,  0.9983,  0.9903,  0.9140],\n",
      "        [-0.6024,  0.8309,  0.6144,  ...,  0.9998,  0.8273, -0.8432],\n",
      "        [ 0.5317,  0.9640,  0.9502,  ..., -0.9994, -0.1773,  0.8744]],\n",
      "       device='cuda:0'), 'SP_LONG_DESC_embeddings': tensor([[-0.8353,  0.9734, -0.9823,  ...,  0.9971,  0.9976, -0.7578],\n",
      "        [ 0.9717,  0.7132,  0.8791,  ...,  0.9911,  0.8676, -0.7440],\n",
      "        [ 0.9719, -0.3351,  0.9141,  ...,  0.9956,  0.9862,  0.3395],\n",
      "        ...,\n",
      "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [-0.1758,  0.9904,  0.8948,  ..., -0.9995, -0.1103,  0.8007]],\n",
      "       device='cuda:0'), 'SP_SHORT_DESC_embeddings': tensor([[-0.6867,  0.9938, -0.9920,  ...,  0.9990,  0.9984, -0.8257],\n",
      "        [ 0.9754,  0.2401,  0.8951,  ...,  0.9970,  0.9942, -0.1940],\n",
      "        [ 0.9910, -0.3897,  0.1733,  ...,  0.9938,  0.9887,  0.4006],\n",
      "        ...,\n",
      "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [-0.3860,  0.9968,  0.8220,  ..., -0.9989, -0.7166,  0.4519]],\n",
      "       device='cuda:0'), 'ORBIS_PROD_SERV_embeddings': tensor([[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [ 0.9728,  0.9936,  0.9686,  ...,  0.9983,  0.9880, -0.1903],\n",
      "        [ 0.9950, -0.8198,  0.8770,  ...,  0.9687, -0.7578, -0.7438],\n",
      "        ...,\n",
      "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [-0.7989,  0.9828,  0.1794,  ...,  0.3016, -0.9985,  0.8141]],\n",
      "       device='cuda:0'), 'ORBIS_OVERVIEW_embeddings': tensor([[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [ 0.8935,  0.9151,  0.7551,  ...,  0.9825, -0.0444, -0.9134],\n",
      "        [ 0.9968, -0.3648,  0.7675,  ...,  0.9925,  0.9506, -0.0904],\n",
      "        ...,\n",
      "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "        [-0.1153,  0.9665, -0.0244,  ..., -0.9805, -0.9381,  0.8755]],\n",
      "       device='cuda:0')}\n",
      "total number of embedding: 5985\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "convert original embeddings to new latent space with trained_ae and trained_clasf\n",
    "'''\n",
    "def safe_inference(model, input_tensor):\n",
    "    '''\n",
    "    Passes the input tensor through the network,\n",
    "    skipping rows containing only NaNs while preserving their original positions in the output.\n",
    "    '''\n",
    "    # Create a mask to identify NaN rows\n",
    "    nan_mask = torch.isnan(input_tensor).all(dim=1)  # True for rows that are fully NaN\n",
    "    \n",
    "    # Extract valid (non-NaN) rows\n",
    "    valid_rows = input_tensor[~nan_mask]  # Select rows where nan_mask is False\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        valid_output = model(valid_rows)\n",
    "\n",
    "    if isinstance(valid_output, tuple):\n",
    "        _, valid_output = valid_output\n",
    "    \n",
    "    # Create an output tensor filled with NaNs\n",
    "    output = torch.full((input_tensor.shape[0], valid_output.shape[1]), float('nan'), device=input_tensor.device)\n",
    "    \n",
    "    # Insert computed values into the non-NaN positions\n",
    "    output[~nan_mask] = valid_output\n",
    "    \n",
    "    return output\n",
    "\n",
    "latent_tensors = {}\n",
    "for col in target_list:\n",
    "    latent_tensors[col] = safe_inference(trained_ae.encoder_net, embedding_tensors[col].to(device))\n",
    "\n",
    "print(latent_tensors)\n",
    "\n",
    "emb_num = 0\n",
    "for col in target_list:\n",
    "    emb_num += len(latent_tensors[col])\n",
    "print(f\"total number of embedding: {emb_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in target_list:\n",
    "    exp_df[col] = latent_tensors[col].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Program_Files\\Anaconda3\\envs\\NLP\\lib\\site-packages\\pandas\\core\\indexes\\base.py:7834: RuntimeWarning: invalid value encountered in cast\n",
      "  values = values.astype(str)\n"
     ]
    }
   ],
   "source": [
    "# exp_df.to_csv(\"./data/embedding_256.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
